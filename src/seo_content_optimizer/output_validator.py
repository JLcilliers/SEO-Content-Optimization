"""
Output validation for LLM-generated content.

This module provides post-generation safety checks to ensure LLM outputs
don't contain blocked industry terms or off-topic content that wasn't
in the original. This is a CRITICAL defense-in-depth layer that catches
any hallucinations or prompt injection that bypassed earlier filters.

The validation works by:
1. Detecting blocked industry terms in LLM output
2. Checking if those terms were present in the original content
3. Flagging or sanitizing content that introduces off-topic industries
"""

import re
from dataclasses import dataclass
from typing import Optional


# HIGH-RISK INDUSTRY TERMS - These should NEVER be introduced by the optimizer
# These are the same as in keyword_filter.py but used for output validation
BLOCKED_TERMS = {
    # Cannabis/Hemp - HIGHEST PRIORITY
    "cannabis", "marijuana", "hemp", "cbd", "thc", "dispensary",
    "cannabidiol", "cannabis-infused", "hemp-derived", "marijuana dispensary",

    # Adult/Gambling/Vice
    "adult", "gambling", "casino", "betting", "poker", "lottery",
    "escort", "porn", "xxx", "adult entertainment",

    # Tobacco/Vape
    "tobacco", "vape", "vaping", "e-cigarette", "e-cig", "nicotine",

    # Firearms/Weapons
    "firearms", "guns", "ammunition", "weapons", "firearm",
    "gun shop", "gun store", "ammo",

    # Alcohol
    "alcohol", "liquor", "wine", "beer", "brewery", "distillery",
    "bar", "tavern", "pub", "liquor store",

    # Spammed Service Verticals
    "hair salon", "nail salon", "barbershop", "spa", "massage",
    "massage therapy", "massage parlor",
    "tattoo", "tattoo parlor", "piercing", "body piercing",
    "tanning", "tanning salon", "beauty salon", "salon",
    "restaurant", "food truck", "cafe", "coffee shop", "bakery",
    "gym", "fitness center", "yoga studio", "pilates", "crossfit",
    "auto repair", "car wash", "mechanic", "auto body",
    "laundromat", "dry cleaning", "laundry",
    "pet grooming", "veterinary", "vet clinic", "pet store",
    "daycare", "childcare", "preschool",
    "florist", "flower shop",
    "convenience store", "pawn shop", "pawnshop",

    # High-Risk Merchant Terms (commonly injected wrongly)
    "high risk merchant", "high-risk merchant", "high risk processing",
    "high-risk processing", "high risk payment", "high-risk payment",
    "merchant account", "merchant services",

    # Crypto/Forex
    "forex", "cryptocurrency", "crypto", "bitcoin", "nft",
    "crypto exchange", "bitcoin payment",

    # Payday/Debt
    "payday loan", "cash advance", "debt collection",
    "payday lender", "title loan",

    # Healthcare (regulated, shouldn't be added)
    "pharmacy", "medical", "healthcare", "doctor", "clinic",
    "dental", "dentist", "chiropractor", "therapy",
    "telehealth", "telemedicine", "prescription",
}

# Single word high-priority blocklist - individual words that are ALWAYS blocked
# These are the most aggressively blocked terms
SINGLE_WORD_BLOCKLIST = {
    "hemp", "cbd", "cannabis", "marijuana", "thc", "dispensary",
    "salon", "spa", "massage", "tattoo",
    "casino", "gambling", "betting",
    "firearm", "firearms", "guns", "weapons",
    "escort", "porn", "adult",
    "vape", "vaping",
}


@dataclass
class ValidationResult:
    """Result of content validation."""
    is_valid: bool
    violations: list[str]
    sanitized_content: Optional[str] = None
    original_content: str = ""

    @property
    def has_violations(self) -> bool:
        return len(self.violations) > 0


def _term_in_text(term: str, text: str) -> bool:
    """Check if a term appears as a whole word/phrase in text."""
    pattern = r'\b' + re.escape(term) + r'\b'
    return bool(re.search(pattern, text, re.IGNORECASE))


def find_blocked_terms(text: str) -> list[str]:
    """
    Find all blocked terms present in text.

    Args:
        text: Text to scan for blocked terms.

    Returns:
        List of blocked terms found (lowercased).
    """
    text_lower = text.lower()
    found = []

    # Check single-word blocklist first (highest priority)
    for term in SINGLE_WORD_BLOCKLIST:
        if _term_in_text(term, text_lower):
            found.append(term)

    # Check full blocked terms list
    for term in BLOCKED_TERMS:
        if term not in SINGLE_WORD_BLOCKLIST and _term_in_text(term, text_lower):
            found.append(term)

    return list(set(found))  # Deduplicate


def validate_output(
    llm_output: str,
    original_content: str,
    context: str = "content",
) -> ValidationResult:
    """
    Validate LLM output for blocked industry terms.

    This is the main validation function. It checks if the LLM output
    contains any blocked terms that weren't in the original content.

    Args:
        llm_output: The content generated by the LLM.
        original_content: The original content before optimization.
        context: Description of what's being validated (for error messages).

    Returns:
        ValidationResult with validity status and any violations found.
    """
    if not llm_output:
        return ValidationResult(
            is_valid=True,
            violations=[],
            original_content=original_content,
        )

    # Find blocked terms in the LLM output
    output_blocked = find_blocked_terms(llm_output)

    if not output_blocked:
        # No blocked terms found - output is valid
        return ValidationResult(
            is_valid=True,
            violations=[],
            original_content=original_content,
        )

    # Check which blocked terms are NEW (not in original)
    original_blocked = find_blocked_terms(original_content) if original_content else []
    original_blocked_set = set(original_blocked)

    # Violations are terms in output that weren't in original
    violations = [term for term in output_blocked if term not in original_blocked_set]

    if not violations:
        # All blocked terms were already in original - this is allowed
        return ValidationResult(
            is_valid=True,
            violations=[],
            original_content=original_content,
        )

    # We have violations - output introduced blocked terms
    return ValidationResult(
        is_valid=False,
        violations=violations,
        original_content=original_content,
    )


def validate_and_fallback(
    llm_output: str,
    original_content: str,
    context: str = "content",
) -> tuple[str, ValidationResult]:
    """
    Validate LLM output and fall back to original if invalid.

    This is the safe wrapper that ensures we never return content
    with newly introduced blocked terms.

    Args:
        llm_output: The content generated by the LLM.
        original_content: The original content before optimization.
        context: Description of what's being validated.

    Returns:
        Tuple of (safe_content, validation_result).
        If validation fails, safe_content will be the original.
    """
    result = validate_output(llm_output, original_content, context)

    if result.is_valid:
        return llm_output, result

    # Validation failed - fall back to original content
    # Log the violation for debugging (in production, this could be sent to monitoring)
    result.sanitized_content = original_content
    return original_content, result


def validate_faq_items(
    faq_items: list[dict[str, str]],
    original_content: str,
) -> tuple[list[dict[str, str]], list[ValidationResult]]:
    """
    Validate FAQ items and filter out any with blocked terms.

    Args:
        faq_items: List of FAQ dicts with 'question' and 'answer' keys.
        original_content: The original page content.

    Returns:
        Tuple of (valid_faqs, all_validation_results).
    """
    valid_faqs = []
    all_results = []

    for i, faq in enumerate(faq_items):
        question = faq.get("question", "")
        answer = faq.get("answer", "")
        combined = f"{question} {answer}"

        result = validate_output(combined, original_content, f"FAQ #{i+1}")
        all_results.append(result)

        if result.is_valid:
            valid_faqs.append(faq)
        # If invalid, we simply skip this FAQ item

    return valid_faqs, all_results


def get_violation_summary(results: list[ValidationResult]) -> dict:
    """
    Generate a summary of validation violations.

    Args:
        results: List of ValidationResult objects.

    Returns:
        Dict with summary statistics.
    """
    total = len(results)
    valid = sum(1 for r in results if r.is_valid)
    invalid = total - valid

    all_violations = []
    for r in results:
        all_violations.extend(r.violations)

    return {
        "total_checked": total,
        "valid_count": valid,
        "invalid_count": invalid,
        "unique_violations": list(set(all_violations)),
        "violation_frequency": {v: all_violations.count(v) for v in set(all_violations)},
    }


# =============================================================================
# CONTENT PRESERVATION VALIDATION
# =============================================================================
# These functions ensure that original content is never deleted during
# optimization. The tool should only ADD content, never remove it.


def validate_content_preservation(
    original: str,
    optimized: str,
    min_preservation_ratio: float = 0.70,
    context: str = "content",
) -> tuple[bool, str]:
    """
    Validate that optimized content preserves the original content.

    This is a CRITICAL check to ensure optimization is ADDITIVE only.
    Content should grow (with keyword insertions), never shrink.

    Args:
        original: Original content before optimization.
        optimized: Content after optimization.
        min_preservation_ratio: Minimum ratio of original length that must be preserved.
            Default 0.85 allows for minor sentence rewording while catching deletions.
        context: Description for error messages.

    Returns:
        Tuple of (is_valid, error_message).
        is_valid is True if content is preserved, False if significant deletion detected.
    """
    if not original:
        return True, ""

    if not optimized:
        return False, f"CRITICAL: {context} was completely deleted (empty output)"

    original_len = len(original.strip())
    optimized_len = len(optimized.strip())

    # Optimized should be >= original (we're adding, not removing)
    if optimized_len >= original_len:
        return True, ""

    # Calculate preservation ratio
    ratio = optimized_len / original_len if original_len > 0 else 0

    if ratio < min_preservation_ratio:
        deletion_pct = (1 - ratio) * 100
        return False, (
            f"CRITICAL: {context} content deleted - "
            f"original: {original_len} chars, optimized: {optimized_len} chars "
            f"({deletion_pct:.1f}% deleted)"
        )

    return True, ""


def validate_block_preservation(
    original_blocks: list,
    optimized_blocks: list,
    context: str = "body",
) -> tuple[bool, str]:
    """
    Validate that all original blocks are preserved in output.

    Args:
        original_blocks: Original content blocks.
        optimized_blocks: Blocks after optimization.
        context: Description for error messages.

    Returns:
        Tuple of (is_valid, error_message).
    """
    if not original_blocks:
        return True, ""

    original_count = len(original_blocks)
    optimized_count = len(optimized_blocks)

    # We should have at least as many blocks (can add, not remove)
    if optimized_count < original_count:
        return False, (
            f"CRITICAL: {context} blocks deleted - "
            f"original: {original_count} blocks, optimized: {optimized_count} blocks"
        )

    return True, ""


# =============================================================================
# ENHANCED BLOCK-LEVEL VALIDATION
# =============================================================================
# These functions provide granular block-level validation to ensure
# every original block is present in the output (with allowed edits).


def _normalize_text_for_matching(text: str) -> str:
    """Normalize text for block matching comparison."""
    if not text:
        return ""
    # Lowercase, collapse whitespace, remove punctuation
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.strip()


def calculate_text_overlap(text1: str, text2: str) -> float:
    """
    Calculate overlap between two texts (0.0 to 1.0).

    Uses word-level Jaccard similarity.
    Public function for use in content preservation validation.
    """
    if not text1 or not text2:
        return 0.0

    words1 = set(_normalize_text_for_matching(text1).split())
    words2 = set(_normalize_text_for_matching(text2).split())

    if not words1 or not words2:
        return 0.0

    intersection = words1 & words2
    union = words1 | words2

    return len(intersection) / len(union) if union else 0.0


def validate_blocks_with_matching(
    original_blocks: list[str],
    optimized_blocks: list[str],
    min_match_threshold: float = 0.50,
) -> tuple[bool, list[str], list[str]]:
    """
    Validate that each original block has a corresponding match in optimized output.

    Uses text overlap matching to handle rewrites while detecting deletions.

    Args:
        original_blocks: List of original text blocks.
        optimized_blocks: List of optimized text blocks.
        min_match_threshold: Minimum overlap to consider a match (0.0-1.0).

    Returns:
        Tuple of (is_valid, unmatched_blocks, match_report).
        unmatched_blocks contains original blocks with no match in output.
    """
    if not original_blocks:
        return True, [], ["No original blocks to validate"]

    unmatched = []
    match_report = []

    for i, orig_block in enumerate(original_blocks):
        if not orig_block or len(orig_block.strip()) < 10:
            # Skip very short blocks
            continue

        # Try to find a matching block in optimized output
        best_match_score = 0.0
        best_match_idx = -1

        for j, opt_block in enumerate(optimized_blocks):
            if not opt_block:
                continue

            overlap = calculate_text_overlap(orig_block, opt_block)
            if overlap > best_match_score:
                best_match_score = overlap
                best_match_idx = j

        if best_match_score >= min_match_threshold:
            match_report.append(
                f"Block {i}: matched to output block {best_match_idx} "
                f"(overlap: {best_match_score:.1%})"
            )
        else:
            unmatched.append(orig_block)
            match_report.append(
                f"Block {i}: NO MATCH (best overlap: {best_match_score:.1%})"
            )

    is_valid = len(unmatched) == 0
    return is_valid, unmatched, match_report


def restore_missing_blocks(
    original_blocks: list[str],
    optimized_blocks: list[str],
    min_match_threshold: float = 0.50,
) -> list[str]:
    """
    Restore any missing original blocks to the optimized output.

    If a block was deleted during optimization, this function
    adds it back (unchanged) to ensure no content loss.

    Args:
        original_blocks: List of original text blocks.
        optimized_blocks: List of optimized text blocks.
        min_match_threshold: Minimum overlap to consider a match.

    Returns:
        List of blocks with missing originals restored.
    """
    is_valid, unmatched, _ = validate_blocks_with_matching(
        original_blocks, optimized_blocks, min_match_threshold
    )

    if is_valid:
        return optimized_blocks

    # Restore unmatched blocks at the end
    result = list(optimized_blocks)
    for block in unmatched:
        result.append(block)

    return result


# =============================================================================
# ANCHOR BLOCK PROTECTION
# =============================================================================
# Anchor blocks contain critical content that must NEVER be lost:
# - Statistics (numbers, percentages)
# - Product specifications
# - Clinical/research data
# - CTA sections


def is_anchor_block(text: str) -> bool:
    """
    Check if a block contains anchor content (stats, numbers, critical info).

    Anchor blocks are content that should NEVER be lost during optimization.

    Args:
        text: Block text to check.

    Returns:
        True if block contains anchor content.
    """
    if not text:
        return False

    text_stripped = text.strip()

    # SHORT NUMERIC BLOCKS: Check these FIRST before length filter
    # Standalone stats like "18+", "235x", "95%", "19x" are critical anchors
    short_anchor_patterns = [
        r'^\d+\+$',  # "18+" style
        r'^\d+(?:\.\d+)?[xX]$',  # "235x", "19x"
        r'^\d+(?:\.\d+)?\s*%$',  # "95%"
        r'^\$\d+(?:,\d{3})*(?:\.\d{2})?$',  # "$100", "$1,000"
        r'^\d+(?:,\d{3})+$',  # "1,000", "10,000"
    ]
    for pattern in short_anchor_patterns:
        if re.match(pattern, text_stripped):
            return True

    # Now apply length filter for longer patterns
    if len(text_stripped) < 20:
        return False

    # Patterns that indicate anchor content
    anchor_patterns = [
        # Statistics and numbers
        r'\d+\s*(?:hours?|hrs?|minutes?|mins?|days?|weeks?|months?|years?)',  # Time durations
        r'\d+(?:\.\d+)?[xX]',  # Multipliers (e.g., "235x", "19x")
        r'\d+(?:\.\d+)?\s*%',  # Percentages
        r'\d+\+',  # "18+" style numbers
        r'\$\d+',  # Prices
        r'\d+,\d{3}',  # Large numbers with commas
        r'(?:FDA|CE|ISO|HIPAA|GDPR)',  # Compliance/certifications
        r'clinical\s+(?:trial|study|research)',  # Clinical data
        r'(?:study|research|trial)\s+(?:shows?|found|demonstrates?)',  # Research claims
        r'(?:AI|artificial intelligence|machine learning)',  # AI/tech claims
        r'(?:chip|processor|technology|sensor)',  # Product tech specs
        # CTA patterns
        r'(?:book|schedule|request|start|get)\s+(?:a|your|an)\s+(?:consultation|appointment|demo|trial)',
        r'(?:call|contact|reach)\s+(?:us|now|today)',
        r'(?:learn|discover|explore)\s+more',
    ]

    text_lower = text.lower()
    for pattern in anchor_patterns:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return True

    return False


def extract_anchor_blocks(blocks: list[str]) -> list[tuple[int, str]]:
    """
    Extract anchor blocks with their indices.

    Args:
        blocks: List of text blocks.

    Returns:
        List of (index, text) tuples for anchor blocks.
    """
    anchors = []
    for i, block in enumerate(blocks):
        if is_anchor_block(block):
            anchors.append((i, block))
    return anchors


def validate_anchor_blocks_preserved(
    original_blocks: list[str],
    optimized_blocks: list[str],
    min_match_threshold: float = 0.40,  # Lower threshold for anchors
) -> tuple[bool, list[str]]:
    """
    Validate that all anchor blocks from original are preserved in optimized.

    Anchor blocks are CRITICAL and should NEVER be lost. This uses a lower
    match threshold because anchor content may be reformatted.

    Args:
        original_blocks: Original text blocks.
        optimized_blocks: Optimized text blocks.
        min_match_threshold: Minimum overlap to consider a match.

    Returns:
        Tuple of (all_preserved, missing_anchor_texts).
    """
    anchors = extract_anchor_blocks(original_blocks)

    if not anchors:
        return True, []

    optimized_text = " ".join(optimized_blocks).lower()
    missing = []

    for idx, anchor_text in anchors:
        # Check if anchor content exists in optimized output
        # First try: direct text match
        if anchor_text.lower()[:100] in optimized_text:
            continue

        # Second try: word overlap matching
        best_overlap = 0.0
        for opt_block in optimized_blocks:
            overlap = calculate_text_overlap(anchor_text, opt_block)
            if overlap > best_overlap:
                best_overlap = overlap

        if best_overlap < min_match_threshold:
            missing.append(anchor_text)

    return len(missing) == 0, missing


def restore_anchor_blocks(
    original_blocks: list[str],
    optimized_blocks: list[str],
) -> list[str]:
    """
    Ensure all anchor blocks from original are present in optimized output.

    If anchor blocks are missing, restore them to prevent content loss.

    Args:
        original_blocks: Original text blocks.
        optimized_blocks: Optimized text blocks.

    Returns:
        Optimized blocks with missing anchors restored.
    """
    all_preserved, missing_anchors = validate_anchor_blocks_preserved(
        original_blocks, optimized_blocks
    )

    if all_preserved:
        return optimized_blocks

    # Restore missing anchor blocks
    result = list(optimized_blocks)
    import sys
    for anchor in missing_anchors:
        print(f"DEBUG: Restoring missing anchor block: {anchor[:80]}...", file=sys.stderr)
        result.append(anchor)

    return result


# =============================================================================
# LOCKED CONTENT VALIDATION
# =============================================================================
# Some content should never be modified (legal, contact, compliance).


LOCKED_CONTENT_PATTERNS = [
    # Legal/compliance
    r"Â©.*\d{4}",  # Copyright notices
    r"all rights reserved",
    r"privacy policy",
    r"terms of service",
    r"terms and conditions",

    # Contact info
    r"\d{3}[-.]?\d{3}[-.]?\d{4}",  # Phone numbers
    r"\d+\s+[A-Z][a-z]+\s+(?:Street|St|Ave|Road|Rd|Dr|Blvd)",  # Addresses

    # Compliance
    r"FDA\s+(?:approved|cleared|registered)",
    r"HIPAA\s+compliant",
    r"ISO\s*\d+",
    r"licensed and insured",
]


def extract_locked_content(text: str) -> list[str]:
    """
    Extract content that should never be modified.

    Args:
        text: Source text.

    Returns:
        List of locked content strings.
    """
    locked = []
    for pattern in LOCKED_CONTENT_PATTERNS:
        matches = re.findall(pattern, text, re.IGNORECASE)
        locked.extend(matches)
    return locked


def validate_locked_content_preserved(
    original: str,
    optimized: str,
) -> tuple[bool, list[str]]:
    """
    Validate that locked content is preserved in output.

    Args:
        original: Original content.
        optimized: Optimized content.

    Returns:
        Tuple of (is_valid, missing_locked_content).
    """
    original_locked = extract_locked_content(original)
    if not original_locked:
        return True, []

    missing = []
    for locked_item in original_locked:
        if locked_item.lower() not in optimized.lower():
            missing.append(locked_item)

    return len(missing) == 0, missing


# =============================================================================
# ORPHAN FRAGMENT DETECTION
# =============================================================================
# Detect and remove stray fragments that don't form complete sentences.
# Common issues: single words ("the", "Transforms"), dangling articles.

# Patterns that indicate orphan fragments
ORPHAN_PATTERNS = [
    # Single articles/prepositions/conjunctions at start/end of blocks
    r'^(?:the|a|an|in|on|at|of|to|for|with|by|from|and|or|but)\s*$',
    # Single verbs/gerunds as fragments
    r'^(?:Transforms|Provides|Enables|Helps|Making|Being|Getting|Using)\s*$',
    # Single tech/category words that appear as broken fragments (not valid standalone)
    r'^(?:AI|Technology|Features|Benefits|Process|Solutions?)\s*$',
    # Incomplete phrases
    r'^(?:and\s+the|or\s+the|the\s+most|is\s+a)\s*$',
    # Just punctuation or whitespace
    r'^[\s\.\,\;\:\-]*$',
]


def is_orphan_fragment(text: str) -> bool:
    """
    Check if a text block is an orphan fragment.

    Args:
        text: Text to check.

    Returns:
        True if the text appears to be an orphan fragment.
    """
    if not text:
        return True

    text = text.strip()

    # Empty or very short
    if len(text) < 3:
        return True

    # Check against orphan patterns
    for pattern in ORPHAN_PATTERNS:
        if re.match(pattern, text, re.IGNORECASE):
            return True

    # EXCEPTION: Stats and anchor content are NOT orphans
    # These are meaningful short blocks (e.g., "18+", "235x", "95%", "19x", "$100")
    stat_pattern = re.compile(
        r'^'
        r'(?:\$[\d,]+(?:\.\d+)?[KkMmBb]?'  # Money: $100, $10,000, $1M
        r'|\d+(?:\.\d+)?[%xX+]'  # Stats: 95%, 235x, 18+
        r'|\d+(?:\.\d+)?[KkMmBb]'  # Large numbers: 100K, 10M
        r'|\d+(?:st|nd|rd|th)'  # Ordinals: 1st, 2nd, 3rd
        r'|\d+(?:\.\d+)?(?:\s*(?:years?|months?|days?|hours?|minutes?|seconds?|weeks?))?'  # Time durations
        r')'
        r'\s*$',
        re.IGNORECASE
    )
    if stat_pattern.match(text):
        return False  # Stats are NOT orphans

    # Single word that's not a complete sentence
    words = text.split()
    if len(words) == 1:
        # Single word is orphan unless it's a valid standalone (e.g., "Yes.", "No.")
        if not text.endswith(('.', '!', '?')):
            return True
        # Even with punctuation, single articles/prepositions/conjunctions are orphans
        word_lower = text.rstrip('.!?').lower()
        if word_lower in {'the', 'a', 'an', 'in', 'on', 'to', 'of', 'and', 'or', 'but', 'for', 'with', 'by', 'from'}:
            return True
        # Single tech/category words are also orphans (even with punctuation)
        if word_lower in {'ai', 'technology', 'features', 'benefits', 'process', 'solution', 'solutions'}:
            return True

    # Two words without ending punctuation
    if len(words) == 2 and not text.endswith(('.', '!', '?')):
        return True

    return False


def remove_orphan_fragments(blocks: list[str]) -> tuple[list[str], list[str]]:
    """
    Remove orphan fragments from a list of text blocks.

    Args:
        blocks: List of text blocks.

    Returns:
        Tuple of (cleaned_blocks, removed_fragments).
    """
    cleaned = []
    removed = []

    for block in blocks:
        if is_orphan_fragment(block):
            removed.append(block)
        else:
            cleaned.append(block)

    return cleaned, removed


def validate_and_preserve(
    llm_output: str,
    original_content: str,
    context: str = "content",
    min_preservation_ratio: float = 0.85,
    min_word_overlap: float = 0.50,
) -> tuple[str, bool, str]:
    """
    Validate LLM output and fall back to original if content was deleted or replaced.

    This combines blocked term validation with content preservation validation.
    If any check fails, returns the original content.

    Validation checks:
    1. No blocked terms introduced
    2. Length ratio preserved (min 85% by default)
    3. Word overlap preserved (min 50% by default) - prevents content replacement

    Args:
        llm_output: The content generated by the LLM.
        original_content: The original content before optimization.
        context: Description for error messages.
        min_preservation_ratio: Minimum ratio of original length to preserve.
        min_word_overlap: Minimum word-level overlap (Jaccard similarity).
            This prevents the LLM from completely replacing content.

    Returns:
        Tuple of (safe_content, is_valid, error_message).
        safe_content will be original_content if validation fails.
    """
    # First check for blocked terms
    term_result = validate_output(llm_output, original_content, context)
    if not term_result.is_valid:
        return original_content, False, f"Blocked terms: {term_result.violations}"

    # Then check content preservation (length ratio)
    is_preserved, preservation_error = validate_content_preservation(
        original_content, llm_output, min_preservation_ratio, context
    )
    if not is_preserved:
        return original_content, False, preservation_error

    # Check word-level overlap to prevent content replacement
    # This catches cases where LLM replaces content with different words of similar length
    overlap = calculate_text_overlap(original_content, llm_output)
    if overlap < min_word_overlap:
        return original_content, False, (
            f"CRITICAL: {context} content replaced - "
            f"word overlap: {overlap:.1%} (minimum: {min_word_overlap:.1%})"
        )

    return llm_output, True, ""
